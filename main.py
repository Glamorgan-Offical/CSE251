import os
import argparse
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

from data.loader import MnistDataloader 
from selector import BaselineSelector, RandomSelector, ClusterSelector

# ========================================================================
# 1-NN Classifier
# ========================================================================
class OneNNClassifier:
    """
    1-NNåˆ†ç±»å™¨
    ä½¿ç”¨é€‰æ‹©å™¨é€‰æ‹©çš„åŸå‹è¿›è¡Œ1æœ€è¿‘é‚»åˆ†ç±»
    """
    
    def __init__(self, selector):
        self.selector = selector
        self.prototypes = None
        self.prototype_labels = None
        
    def fit(self, X_train, y_train):

        self.selector.fit(X_train, y_train)
        self.prototypes = self.selector.prototypes
        self.prototype_labels = self.selector.prototype_labels

        return self
    
    def predict(self, X_test):
        predictions = []
        # æ³¨æ„ï¼šå¯¹äºå¤§å‹æµ‹è¯•é›†ï¼Œè¿™é‡Œå¯ä»¥ç”¨çŸ©é˜µè¿ç®—ä¼˜åŒ–ï¼Œ
        # ä½†ä¸ºäº†ä»£ç æ¸…æ™°å’Œå†…å­˜è€ƒè™‘ï¼Œå¾ªç¯ä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ï¼ˆä¼šæ…¢ä¸€äº›ï¼‰
        for x in X_test:
            distances = np.linalg.norm(self.prototypes - x, axis=1)
            nearest_idx = np.argmin(distances)
            predictions.append(self.prototype_labels[nearest_idx])
        
        return np.array(predictions)
    
    def get_info(self):
        return self.selector.get_info()


# ========================================================================
# Evaluation Functions
# ========================================================================
def evaluate_classifier(classifier, X_test, y_test, verbose=True):
    """
    è¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½
    
    Args:
        classifier: 1-NNåˆ†ç±»å™¨å®ä¾‹
        X_test: æµ‹è¯•æ•°æ®
        y_test: æµ‹è¯•æ ‡ç­¾
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        dict: åŒ…å«å‡†ç¡®ç‡ã€é¢„æµ‹æ—¶é—´ç­‰æŒ‡æ ‡
    """
    # é¢„æµ‹
    start_time = time.time()
    y_pred = classifier.predict(X_test)
    prediction_time = time.time() - start_time
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(y_test, y_pred)
    
    results = {
        'accuracy': accuracy,
        'prediction_time': prediction_time,
        'avg_time_per_sample': prediction_time / len(X_test),
        'y_pred': y_pred,
    }
    
    if verbose:
        info = classifier.get_info()
        print(f"\n{'='*60}")
        print(f"Selector: {info['name']}")
        print(f"Num Prototypes: {info['num_prototypes']}")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Prediction Time: {prediction_time:.2f}s")
        print(f"Time per sample: {prediction_time/len(X_test)*1000:.2f}ms")
        if info['selection_time']:
            print(f"Selection Time: {info['selection_time']:.2f}s")
        print(f"{'='*60}\n")
    
    return results


# ========================================================================
# Visualization Functions
# ========================================================================
def plot_comparison_results(results_dict, save_path=None):
    """ç»˜åˆ¶æ–¹æ³•å¯¹æ¯”ç»“æœ"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    methods = list(results_dict.keys())
    accuracies = [results_dict[m]['accuracy'] for m in methods]
    times = [results_dict[m]['prediction_time'] for m in methods]
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    axes[0].bar(methods, accuracies, color='steelblue', alpha=0.7)
    axes[0].set_ylabel('Accuracy', fontsize=12)
    axes[0].set_title('Classification Accuracy Comparison', fontsize=14)
    axes[0].set_ylim([min(accuracies)*0.95, 1.0])
    axes[0].grid(axis='y', alpha=0.3)
    plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    for i, (method, acc) in enumerate(zip(methods, accuracies)):
        axes[0].text(i, acc + 0.01, f'{acc:.3f}', ha='center', fontsize=10)
    
    # é¢„æµ‹æ—¶é—´å¯¹æ¯”
    axes[1].bar(methods, times, color='coral', alpha=0.7)
    axes[1].set_ylabel('Prediction Time (s)', fontsize=12)
    axes[1].set_title('Prediction Time Comparison', fontsize=14)
    axes[1].grid(axis='y', alpha=0.3)
    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    for i, (method, t) in enumerate(zip(methods, times)):
        axes[1].text(i, t + max(times)*0.02, f'{t:.1f}s', ha='center', fontsize=10)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nComparison plot saved as {save_path}")
    plt.show()


def plot_prototype_count_analysis(prototype_counts, accuracies, times, save_path=None):
    """ç»˜åˆ¶åŸå‹æ•°é‡åˆ†æ"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å‡†ç¡®ç‡ vs åŸå‹æ•°é‡
    axes[0].plot(prototype_counts, accuracies, marker='o', linewidth=2, markersize=8, color='steelblue')
    axes[0].set_xlabel('Number of Prototypes per Class', fontsize=12)
    axes[0].set_ylabel('Accuracy', fontsize=12)
    axes[0].set_title('Accuracy vs Prototype Count', fontsize=14)
    axes[0].grid(True, alpha=0.3)
    
    # é¢„æµ‹æ—¶é—´ vs åŸå‹æ•°é‡
    axes[1].plot(prototype_counts, times, marker='s', linewidth=2, markersize=8, color='coral')
    axes[1].set_xlabel('Number of Prototypes per Class', fontsize=12)
    axes[1].set_ylabel('Prediction Time (s)', fontsize=12)
    axes[1].set_title('Prediction Time vs Prototype Count', fontsize=14)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nPrototype count analysis plot saved as {save_path}")
    plt.show()


# ========================================================================
# Experiment Functions
# ========================================================================
def run_comparison_experiment(X_train, X_test, y_train, y_test, num_prototypes=50):
    """
    è¿è¡Œå¯¹æ¯”å®éªŒï¼šFull (Baseline) vs Random vs PCA+KMeans
    """
    print("="*70)
    print("MNIST Prototype Selection - Method Comparison")
    print("="*70)
    
    # å®šä¹‰é€‰æ‹©å™¨
    print(f"\n[1/3] Initializing selectors (prototypes per class: {num_prototypes})...")
    
    selectors = {
        'Baseline': BaselineSelector(),
        'Random_Selection': RandomSelector(
            num_prototypes_per_class=num_prototypes,
            random_state=42
        ),
        'Cluster': ClusterSelector(
            num_prototypes_per_class=num_prototypes,
            random_state=42,
            pca_components=100
        ),
    }
    
    # è®­ç»ƒå’Œè¯„ä¼°
    print("\n[2/3] Training and evaluating 1-NN classifiers...")
    results = {}
    
    for name, selector in selectors.items():
        print(f"\n>>> Processing {name}...")
        classifier = OneNNClassifier(selector)
        classifier.fit(X_train, y_train)
        results[name] = evaluate_classifier(classifier, X_test, y_test, verbose=True)
    
    # å¯è§†åŒ–ç»“æœ
    print("\n[3/3] Generating visualizations...")
    os.makedirs('results/figures', exist_ok=True)
    plot_comparison_results(
        results,
        save_path=f'results/figures/method_comparison_{num_prototypes}ppc.png'
    )
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*70)
    print("EXPERIMENT SUMMARY")
    print("="*70)
    baseline_time = results['Baseline']['prediction_time']
    for name, result in results.items():
        print(f"\n{name}:")
        print(f"  - Accuracy: {result['accuracy']:.4f}")
        print(f"  - Prediction Time: {result['prediction_time']:.2f}s")
        if name != 'Baseline':
            speedup = baseline_time / result['prediction_time']
            print(f"  - Speedup vs Baseline: {speedup:.2f}x")
    
    return results


def run_parameter_tuning(X_train, X_test, y_train, y_test):
    """
    å‚æ•°è°ƒä¼˜å®éªŒï¼šæµ‹è¯•ä¸åŒæ•°é‡çš„åŸå‹
    """
    print("\n" + "="*70)
    print("MNIST Prototype Selection - Parameter Tuning")
    print("="*70)
    
    prototype_counts = [10, 20, 50, 100, 200]
    print(f"\nTesting prototype counts: {prototype_counts}")
    
    accuracies = []
    times = []
    
    for count in prototype_counts:
        print(f"\n{'='*60}")
        print(f"Testing with {count} prototypes per class...")
        print(f"{'='*60}")
        
        # åˆ›å»ºé€‰æ‹©å™¨å’Œåˆ†ç±»å™¨
        selector = ClusterSelector(
            num_prototypes_per_class=count,
            random_state=42,
            pca_components=100
        )
        classifier = OneNNClassifier(selector)
        
        # è®­ç»ƒå’Œè¯„ä¼°
        classifier.fit(X_train, y_train)
        result = evaluate_classifier(classifier, X_test, y_test, verbose=True)
        
        accuracies.append(result['accuracy'])
        times.append(result['prediction_time'])
    
    # å¯è§†åŒ–
    os.makedirs('results/figures', exist_ok=True)
    plot_prototype_count_analysis(
        prototype_counts,
        accuracies,
        times,
        save_path='results/figures/parameter_tuning.png'
    )
    
    # æ‰“å°æœ€ä½³ç»“æœ
    best_idx = np.argmax(accuracies)
    print("\n" + "="*70)
    print("PARAMETER TUNING SUMMARY")
    print("="*70)
    print(f"\nBest accuracy: {accuracies[best_idx]:.4f}")
    print(f"Best prototype count: {prototype_counts[best_idx]} per class")
    print(f"Total prototypes: {prototype_counts[best_idx] * 10}")
    
    return prototype_counts, accuracies, times


# ========================================================================
# Main Function
# ========================================================================
def main():
    """ä¸»å‡½æ•°"""
    # å‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='MNIST Prototype Selection with 1-NN Classifier')
    parser.add_argument('--experiment', type=str, default='comparison',
                        choices=['comparison', 'tuning', 'both'],
                        help='Experiment type: comparison (default), tuning, or both')
    parser.add_argument('--num_prototypes', type=int, default=50,
                        help='Number of prototypes per class for comparison experiment')
    parser.add_argument('--quick_test', action='store_true',
                        help='Use subset of test data for quick testing')
    
    args = parser.parse_args()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    training_images_filepath = 'data/train-images.idx3-ubyte'
    training_labels_filepath = 'data/train-labels.idx1-ubyte'
    test_images_filepath = 'data/t10k-images.idx3-ubyte'
    test_labels_filepath = 'data/t10k-labels.idx1-ubyte'
    
    # åŠ è½½æ•°æ®
    print("\n" + "="*70)
    print("Loading MNIST Dataset")
    print("="*70)
    
    start_time = time.time()
    
    # å®ä¾‹åŒ–åŠ è½½å™¨
    mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath,
                                       test_images_filepath, test_labels_filepath)
    (x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–
    X_train = np.array(x_train).reshape(len(x_train), -1) / 255.0
    X_test = np.array(x_test).reshape(len(x_test), -1) / 255.0
    y_train = np.array(y_train)
    y_test = np.array(y_test)
    
    elapsed_time = time.time() - start_time
    print(f"Data loaded in: {elapsed_time:.2f} seconds")
    print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    if args.quick_test:
        print("\nâš ï¸  Quick test mode: using 1000 test samples")
        X_test = X_test[:1000]
        y_test = y_test[:1000]
    
    # è¿è¡Œå®éªŒ
    if args.experiment == 'comparison':
        run_comparison_experiment(X_train, X_test, y_train, y_test, args.num_prototypes)
        
    elif args.experiment == 'tuning':
        run_parameter_tuning(X_train, X_test, y_train, y_test)
        
    elif args.experiment == 'both':
        run_comparison_experiment(X_train, X_test, y_train, y_test, args.num_prototypes)
        run_parameter_tuning(X_train, X_test, y_train, y_test)
    
    print("\n" + "="*70)
    print("âœ… All experiments completed successfully!")
    print("ğŸ“Š Results saved to: ./results/figures/")
    print("="*70)


if __name__ == "__main__":
    main()